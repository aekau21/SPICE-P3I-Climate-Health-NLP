---
title: "FP Draft 1"
author: "Anson"
date: "2023-06-14"
output: html_document
---
Goal: Compare super claims 1, 3, and 5. 
1 : Not Happening 
3: Climate Impacts Not Bad
5: Science/Scientist Not Reliable


```{r setup, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(jsonlite) # allows us to read in json files
library(tidyverse) # allows us to do lots of data manipulation and basic data science
library(here) # allows us to cut out long file paths (ex. "users/connor/dowloads/etc")
library(forcats) # 
library(tidytext) # allows us to tokenize data 
library(dplyr) # allows us to manipulate dataframes
library(stringr) # allows us to count the number of words in a cell
library(quanteda) # allows us to tokenize data
library(quanteda.textplots) # allows us to make network plots
library(gridExtra) # allows us to combine multiple plots into 1
library(wordcloud) # allows us to generate word clouds
library(fmsb)
library(plotly)
library(ggthemes)
library(tm)
library(syuzhet)
library(wordcloud2)
```

```{r Read In}
nature_analysis <- read_csv(here("data/training.csv"))
```

#Super Claim 1 Not Happening
Filter() to select super claim 1
```{r}
na_1 <- nature_analysis %>%
  filter(str_detect(claim, "1_"))
```

Add word_count column using mutate()
```{r}
na_1 <- na_1 %>% 
  mutate(word_count = str_count(na_1$text, "\\S+"))
```

Distribution visual, geom_histogram
```{r Histogram 1}
ggplot(na_1, aes(x = word_count, fill = claim)) +
  geom_histogram(bins = 67, color = "black") +
  theme_wsj()+
  theme(text = element_text(family = "Menlo-Bold", size = 12),
        legend.title = element_text(family = "Menlo-Bold", size = 12)) +
  labs(title = "Distribution of Claims", 
       subtitle = "Claim 1")

```

Tokenize using unnest_tokens() to seprate text into words
```{r}
na_1_tokenized <- na_1 %>% 
  unnest_tokens(words, text)

na_1_tokenized <- na_1_tokenized %>% 
  count(words) %>% 
  arrange(desc(n))
```

Filter() out stopwords()
```{r}
na_1_tokenized <- na_1_tokenized %>% 
  filter(!words %in% stopwords("english"))
```

Word Cloud 1
```{r Word Cloud 1, warning=FALSE}
wordcloud(na_1_tokenized$words, freq = na_1_tokenized$n, max.words = 200, min.freq = 5, random.order = FALSE, colors = brewer.pal(12, "Paired"))
```

```{r Frequencey Matrix 1}
na_1_corpus <- corpus(na_1$text)

toks <- na_1_corpus %>%
    tokens(remove_punct = TRUE) %>%
    tokens_tolower() %>%
    tokens_remove(pattern = stopwords("english"), padding = FALSE)


fcmat <- fcm(toks, context = "window", tri = FALSE)

feat <- names(topfeatures(fcmat, 30))

fcm_select(fcmat, pattern = feat) %>%
    textplot_network(min_freq = 0.5)
```

```{r bigram 1}
na_1_claims <- na_1 %>% 
  select(text)

ngrams <- na_1_claims %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2)


ngrams <- ngrams %>% 
 separate(bigram, c("word1", "word2"), sep = " ") 

ngrams <- ngrams %>%
  filter(!word1 %in% stop_words$word) %>%
      filter(!word2 %in% stop_words$word)


ngrams <- ngrams %>%
  unite(bigram, word1, word2, sep=" ")

ngrams_counts <- ngrams %>% 
  count(bigram, sort = TRUE)
```

```{r Sentiment 1}
#nrc_lexicon <- get_sentiments("nrc")

```





#Super Claim 3 Climate Impacts Not Bad
FIlter() for super claim 3
```{r}
na_3 <- nature_analysis %>%
  filter(str_detect(claim, "3_"))
```

Add word_count column using mutate()
```{r}
na_3 <- na_3 %>% 
  mutate(word_count = str_count(na_3$text, "\\S+"))
```

Distribution visual, geom_histogram
```{r Histogram 3}
ggplot(na_3, aes(x = word_count, fill = claim)) +
  geom_histogram(bins = 67, color = "black") +
  theme_wsj()+
  theme(text = element_text(family = "Menlo-Bold", size = 12),
        legend.title = element_text(family = "Menlo-Bold", size = 12)) +
  labs(title = "Distribution of Claims", 
       subtitle = "Claim 3")

```

Tokenize using unnest_tokens()
```{r}
na_3_tokenized <- na_3 %>% 
  unnest_tokens(words, text)

na_3_tokenized <- na_3_tokenized %>% 
  count(words) %>% 
  arrange(desc(n))
```

Filter() out stopwords()
```{r}
na_3_tokenized <- na_3_tokenized %>%
  anti_join(stop_words, by = c("words" = "word")) %>%
  filter(!words %in% c("et", "al", "2"))
```

Word Cloud 3
```{r Word Cloud 3, warning=FALSE}
wordcloud(na_3_tokenized$words, freq = na_3_tokenized$n, max.words = 200, min.freq = 5, random.order = FALSE, random.color = FALSE, colors = brewer.pal(12, "Paired"))
```


```{r Frequency Matrix 3}
na_3_corpus <- corpus(na_3$text)

toks <- na_3_corpus %>%
    tokens(remove_punct = TRUE) %>%
    tokens_tolower() %>%
    tokens_remove(pattern = stopwords("english"), padding = FALSE)


fcmat <- fcm(toks, context = "window", tri = FALSE)

feat <- names(topfeatures(fcmat, 30))

fcm_select(fcmat, pattern = feat) %>%
    textplot_network(min_freq = 0.5)
```

```{r bigram 3}
na_3_claims <- na_3 %>% 
  select(text)

ngrams_3 <- na_3_claims %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

ngrams_3 <- ngrams_3 %>% 
 separate(bigram, c("word1", "word2"), sep = " ")

ngrams_3 <- ngrams_3 %>% 
  filter(!word1 %in% stop_words$word) %>% 
         filter(!word2 %in% stop_words$word)

ngrams_3 <- ngrams_3 %>% 
  unite(bigrams, word1, word2, sep = " ")

ngrams_3 <- ngrams_3 %>% 
  count(bigrams, sort = TRUE)

```








#Super Claim 5 Science/Scientist Not Reliable
FIlter() for super claim 5
```{r}
na_5 <- nature_analysis %>%
  filter(str_detect(claim, "5_"))
```

Add word_count column using mutate()
```{r}
na_5 <- na_5 %>% 
  mutate(word_count = str_count(na_5$text, "\\S+"))
```

Distribution visual, geom_histogram
```{r Histogram 3}
ggplot(na_5, aes(x = word_count, fill = claim)) +
  geom_histogram(bins = 67, color = "black") +
  theme_wsj()+
  theme(text = element_text(family = "Menlo-Bold", size = 12),
        legend.title = element_text(family = "Menlo-Bold", size = 12)) +
  labs(title = "Distribution of Claims", 
       subtitle = "Claim 5")

```

Tokenize using unnest_tokens()
```{r}
na_5_tokenzied <- nature_analysis %>% 
  unnest_tokens(words, text)

na_5_tokenzied <- na_5_tokenzied %>% 
  count(words) %>% 
  arrange(desc(n))
  
```

Filter() out stopwords()
```{r}
na_5_tokenzied <- na_5_tokenzied %>% 
  filter(!words %in% stopwords("english"))
```

Word Cloud 5
```{r Word Cloud 5, warning=FALSE}
wordcloud(na_5_tokenzied$words, freq = na_5_tokenzied$n, max.words = 200, min.freq = 5, random.order = FALSE, random.color = FALSE, color = brewer.pal(12, "Paired"))
```


```{r Frequency Matrix 5}
na_5_corpus <- corpus(na_5$text)

toks <- na_5_corpus %>%
    tokens(remove_punct = TRUE) %>%
    tokens_tolower() %>%
    tokens_remove(pattern = stopwords("english"), padding = FALSE)


fcmat <- fcm(toks, context = "window", tri = FALSE)

feat <- names(topfeatures(fcmat, 30))

fcm_select(fcmat, pattern = feat) %>%
    textplot_network(min_freq = 0.5)
```

```{r bigram 5}
na_5_claims <- na_5 %>% 
  select(text)

ngrams_5 <- na_5_claims %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

ngrams_5 <- ngrams_5 %>% 
  separate(bigram, c("word1", "word2"), sep = " ")

ngrams_5 <- ngrams_5 %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word)

ngrams_5 <- ngrams_5 %>% 
  unite(bigram, word1, word2, sep = " ")

ngrams_5 <- ngrams_5 %>% 
  count(bigram, sort = TRUE)

```







#Comparison Cloud code
```{r prep}
na_1_matp <- na_1_claims %>% 
  select(text)

na_3_matp <- na_3_claims %>% 
  select(text)

na_5_matp <- na_5_claims %>% 
  select(text)

na_1_matrix <- as.matrix.data.frame(na_1_matp)
na_3_matrix <- as.matrix.data.frame(na_3_matp)
na_5_matrix <- as.matrix.data.frame(na_5_matp)

na_1_text <- apply(na_1_matrix, 1, toString)
na_3_text <- apply(na_3_matrix, 1, toString)
na_5_text <- apply(na_5_matrix, 1, toString)






na_list <- list(na_1_text, na_3_text, na_5_text)

na_list <- lapply(na_list, as.vector.data.frame)
unlist(na_list)

```


```{r Comparison Cloud}
if (require(tm)) {
  # Replace the following code with your own text data
  texts <- (na_list)
  
  # Create a corpus from the text data
  corp <- Corpus(VectorSource(texts))
  
  # Preprocess the corpus
  corp <- tm_map(corp, removePunctuation)
  corp <- tm_map(corp, content_transformer(tolower))
  corp <- tm_map(corp, removeNumbers)
  corp <- tm_map(corp, removeWords, stopwords())
  
  # Create the term document matrix
  term.matrix <- DocumentTermMatrix(corp)
  term.matrix <- as.matrix(term.matrix)
  
  # Assign column names to the matrix
  colnames(term.matrix) <- paste0("Document ", 1:ncol(term.matrix))
  
  # Generate the word cloud
  comparison.cloud(term.matrix, max.words = 200, random.order = FALSE,)
  comparison.cloud(term.matrix, max.words = 200, random.order = FALSE,
                   title.colors = c("red", "blue"), title.bg.colors = c("grey40", "grey70"))
  comparison.cloud(term.matrix, max.words = 200, random.order = FALSE,
                   match.colors = TRUE)
  break
}

```

