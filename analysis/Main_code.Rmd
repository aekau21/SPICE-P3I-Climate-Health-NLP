---
title: "FP Draft 1"
author: "Anson"
date: "2023-06-14"
output:
  html_document: default
  pdf_document: default
---
Goal: Compare super claims 1, 3, and 5. 
1 : Not Happening 
3: Climate Impacts Not Bad
5: Science/Scientist Not Reliable


```{r setup, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(jsonlite) # allows us to read in json files
library(tidyverse) # allows us to do lots of data manipulation and basic data science
library(here) # allows us to cut out long file paths (ex. "users/connor/dowloads/etc")
library(forcats) # 
library(tidytext) # allows us to tokenize data 
library(dplyr) # allows us to manipulate dataframes
library(stringr) # allows us to count the number of words in a cell
library(quanteda) # allows us to tokenize data
library(quanteda.textplots) # allows us to make network plots
library(gridExtra) # allows us to combine multiple plots into 1
library(wordcloud) # allows us to generate word clouds
library(fmsb)
library(plotly) #interactive ggplot graphs
library(ggthemes) # more themes for ggplot
library(tm) #for textmining corpus(), removePunctuation()
library(syuzhet) # for sentiment analysis, getNrc()
library(wordcloud2) # for comparison clouds 
library(plotrix) # for pyramid plots
library(RColorBrewer) # for more color palettes

```

```{r Read In}
nature_analysis <- read_csv(here("data/training.csv"))
```

#Super Claim 1 Not Happening
Filter() to select super claim 1
```{r}
na_1 <- nature_analysis %>%
  filter(str_detect(claim, "1_"))
```

Add word_count column using mutate()
```{r}
na_1 <- na_1 %>% 
  mutate(word_count = str_count(na_1$text, "\\S+"))
```

Distribution visual, geom_histogram
```{r Histogram 1}
ggplot(na_1, aes(x = word_count, fill = claim)) +
  geom_histogram(bins = 67, color = "black") +
  theme(text = element_text(family = "Menlo-Bold", size = 12),
        legend.title = element_text(family = "Menlo-Bold", size = 12)) +
  labs(title = "Distribution of Claims", 
       subtitle = "Claim 1")

```

Tokenize using unnest_tokens() to seprate text into words
```{r}
na_1_tokenized <- na_1 %>% 
  unnest_tokens(words, text)

na_1_tokenized <- na_1_tokenized %>% 
  count(words) %>% 
  arrange(desc(n))
```

Filter() out stopwords()
```{r}
na_1_tokenized <- na_1_tokenized %>% 
  filter(!words %in% stopwords("english"))
```

Word Cloud 1
```{r Word Cloud 1, warning=FALSE}
wordcloud(na_1_tokenized$words, freq = na_1_tokenized$n, max.words = 200, min.freq = 5, random.order = FALSE, colors = c("royalblue1","seagreen2", "orangered"), family = "Avenir")

```

```{r Frequencey Matrix 1}
na_1_corpus <- corpus(na_1$text)

toks <- na_1_corpus %>%
    tokens(remove_punct = TRUE) %>%
    tokens_tolower() %>%
    tokens_remove(pattern = stopwords("english"), padding = FALSE)


fcmat <- fcm(toks, context = "window", tri = FALSE)

feat <- names(topfeatures(fcmat, 30))

fcm_select(fcmat, pattern = feat) %>%
    textplot_network(min_freq = 0.5)
```

```{r bigram 1}
na_1_claims <- na_1 %>% 
  select(text)

ngrams <- na_1_claims %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2)


ngrams <- ngrams %>% 
 separate(bigram, c("word1", "word2"), sep = " ") 

ngrams <- ngrams %>%
  filter(!word1 %in% stop_words$word) %>%
      filter(!word2 %in% stop_words$word)


ngrams <- ngrams %>%
  unite(bigram, word1, word2, sep=" ")

ngrams_1 <- ngrams %>% 
  count(bigram, sort = TRUE)
```

```{r Sentiment 1}
na_1_claims_vector <- as.character(na_1_claims$text)
na_1_sentiment <- get_nrc_sentiment(na_1_claims_vector)

na_1_sentiment_score <- data.frame(colSums(na_1_sentiment[,]))

names(na_1_sentiment_score) <- 'score'

na_1_sentiment_score <- cbind("sentiment" = rownames(na_1_sentiment_score), na_1_sentiment_score)

#rownames(training_sentiment_score) <- NULL

ggplot(na_1_sentiment_score, aes(x = sentiment, y = score)) +
  geom_bar(aes(fill = sentiment), stat="identity") +
  labs(x = "Sentiments", y = "Scores", title = "Sentiment for Super Claim 1")

```





#Super Claim 3 Climate Impacts Not Bad
Filter() for super claim 3
```{r}
na_3 <- nature_analysis %>%
  filter(str_detect(claim, "3_"))
```

Add word_count column using mutate()
```{r}
na_3 <- na_3 %>% 
  mutate(word_count = str_count(na_3$text, "\\S+"))
```

Distribution visual, geom_histogram
```{r Histogram 3}
ggplot(na_3, aes(x = word_count, fill = claim)) +
  geom_histogram(bins = 67, color = "black") +
  theme(text = element_text(family = "Menlo-Bold", size = 12),
        legend.title = element_text(family = "Menlo-Bold", size = 12)) +
  labs(title = "Distribution of Claims", 
       subtitle = "Claim 3")

```

Tokenize using unnest_tokens()
```{r}
na_3_tokenized <- na_3 %>% 
  unnest_tokens(words, text)

na_3_tokenized <- na_3_tokenized %>% 
  count(words) %>% 
  arrange(desc(n))
```

Filter() out stopwords()
```{r}
na_3_tokenized <- na_3_tokenized %>%
  anti_join(stop_words, by = c("words" = "word")) %>%
  filter(!words %in% c("et", "al", "2"))
```

Word Cloud 3
```{r Word Cloud 3, warning=FALSE}
wordcloud(na_3_tokenized$words, freq = na_3_tokenized$n, max.words = 200, min.freq = 5, random.order = FALSE, random.color = FALSE, colors = brewer.pal(12, "Paired"))
```

```{r Frequency Matrix 3}
na_3_corpus <- corpus(na_3$text)

toks <- na_3_corpus %>%
    tokens(remove_punct = TRUE) %>%
    tokens_tolower() %>%
    tokens_remove(pattern = stopwords("english"), padding = FALSE)


fcmat <- fcm(toks, context = "window", tri = FALSE)

feat <- names(topfeatures(fcmat, 30))

fcm_select(fcmat, pattern = feat) %>%
    textplot_network(min_freq = 0.5)
```

```{r bigram 3}
na_3_claims <- na_3 %>% 
  select(text)

ngrams_3 <- na_3_claims %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

ngrams_3 <- ngrams_3 %>% 
 separate(bigram, c("word1", "word2"), sep = " ")

ngrams_3 <- ngrams_3 %>% 
  filter(!word1 %in% stop_words$word) %>% 
         filter(!word2 %in% stop_words$word)

ngrams_3 <- ngrams_3 %>% 
  unite(bigrams, word1, word2, sep = " ")

ngrams_3 <- ngrams_3 %>% 
  count(bigrams, sort = TRUE)

```

```{r Sentiment 3}
na_3_claims_vector <- as.character(na_3_claims$text)
na_3_sentiment <- get_nrc_sentiment(na_3_claims_vector)

na_3_sentiment_score <- data.frame(colSums(na_3_sentiment[,]))

names(na_3_sentiment_score) <- 'score'

na_3_sentiment_score <- cbind("sentiment"=rownames(na_3_sentiment_score), na_3_sentiment_score)

#rownames(training_sentiment_score) <- NULL

ggplot(na_3_sentiment_score, aes(x = sentiment, y = score)) +
  geom_bar(aes(fill = sentiment), stat="identity") +
  labs(x = "Sentiments", y = "Scores", title = "Sentiment for Super Claim 3")
```





#Super Claim 5 Science/Scientist Not Reliable
Filter() for super claim 5
```{r}
na_5 <- nature_analysis %>%
  filter(str_detect(claim, "5_"))
```

Add word_count column using mutate()
```{r}
na_5 <- na_5 %>% 
  mutate(word_count = str_count(na_5$text, "\\S+"))
```

Distribution visual, geom_histogram
```{r Histogram 5}
ggplot(na_5, aes(x = word_count, fill = claim)) +
  geom_histogram(bins = 67, color = "black") +
  theme(text = element_text(family = "Menlo-Bold", size = 12),
        legend.title = element_text(family = "Menlo-Bold", size = 12)) +
  labs(title = "Distribution of Claims", 
       subtitle = "Claim 5")

```

Tokenize using unnest_tokens()
```{r}
na_5_tokenized <- nature_analysis %>% 
  unnest_tokens(words, text)

na_5_tokenized <- na_5_tokenized %>% 
  count(words) %>% 
  arrange(desc(n))
  
```

Filter() out stopwords()
```{r}
na_5_tokenized <- na_5_tokenized %>% 
  filter(!words %in% stopwords("english"))
```

Word Cloud 5
```{r Word Cloud 5, warning=FALSE}
wordcloud(na_5_tokenized$words, freq = na_5_tokenized$n, max.words = 200, min.freq = 5, random.order = FALSE, random.color = FALSE, color = brewer.pal(12, "Paired"))
```


```{r Frequency Matrix 5}
na_5_corpus <- corpus(na_5$text)

toks <- na_5_corpus %>%
    tokens(remove_punct = TRUE) %>%
    tokens_tolower() %>%
    tokens_remove(pattern = stopwords("english"), padding = FALSE)


fcmat <- fcm(toks, context = "window", tri = FALSE)

feat <- names(topfeatures(fcmat, 30))

fcm_select(fcmat, pattern = feat) %>%
    textplot_network(min_freq = 0.5)
```

```{r bigram 5}
na_5_claims <- na_5 %>% 
  select(text)

ngrams_5 <- na_5_claims %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

ngrams_5 <- ngrams_5 %>% 
  separate(bigram, c("word1", "word2"), sep = " ")

ngrams_5 <- ngrams_5 %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word)

ngrams_5 <- ngrams_5 %>% 
  unite(bigram, word1, word2, sep = " ")

ngrams_5 <- ngrams_5 %>% 
  count(bigram, sort = TRUE)

```

```{r Sentiment 5}
na_5_claims_vector <- as.character(na_5_claims$text)
na_5_claims_sentiment <- get_nrc_sentiment(na_5_claims_vector)

na_5_claims_sentiment_score <- data.frame(colSums(na_5_claims_sentiment[,]))

names(na_5_claims_sentiment_score) <- 'score'

na_5_claims_sentiment_score <- cbind("sentiment" = rownames(na_5_claims_sentiment_score), na_5_claims_sentiment_score)

#rownames(training_sentiment_score) <- NULL

ggplot(na_5_claims_sentiment_score, aes(sentiment, score)) +
  geom_bar(aes(fill = sentiment), stat = "identity") +
  labs(x = "Sentiment", y = "Score", title = "Sentiment for Super Claim 5")

```

#Super Claim 5_1
Filter() for super claim 5
```{r}
na_5_1 <- nature_analysis %>%
  filter(str_detect(claim, "5_1"))
```

Add word_count column using mutate()
```{r}
na_5_1 <- na_5_1 %>% 
  mutate(word_count = str_count(na_5_1$text, "\\S+"))
```

Tokenize using unnest_tokens()
```{r}
na_5_1_tokenized <- nature_analysis %>% 
  unnest_tokens(words, text)

na_5_1_tokenized <- na_5_1_tokenized %>% 
  count(words) %>% 
  arrange(desc(n))
  
```

Filter() out stopwords()
```{r}
na_5_1_tokenized <- na_5_1_tokenized %>% 
  filter(!words %in% stopwords("english"))
```

```{r Word Cloud 5_1, warning=FALSE}
wordcloud(na_5_1_tokenized$words, freq = na_5_1_tokenized$n, max.words = 200, min.freq = 5, random.order = FALSE, random.color = FALSE, color = brewer.pal(12, "Paired"))
```

```{r frequencey matrix 5_1}
na_5_1_corpus <- corpus(na_5_1$text)

toks <- na_5_1_corpus %>%
    tokens(remove_punct = TRUE) %>%
    tokens_tolower() %>%
    tokens_remove(pattern = stopwords("english"), padding = FALSE)


fcmat <- fcm(toks, context = "window", tri = FALSE)

feat <- names(topfeatures(fcmat, 30))

fcm_select(fcmat, pattern = feat) %>%
    textplot_network(min_freq = 0.5)
```

```{r bigram 5_1}
na_5_1_claims <- na_5_1 %>% 
  select(text)

ngrams_5_1 <- na_5_1_claims %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

ngrams_5_1 <- ngrams_5_1 %>% 
  separate(bigram, c("word1", "word2"), sep = " ")

ngrams_5_1 <- ngrams_5_1 %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word)

ngrams_5_1 <- ngrams_5_1 %>% 
  unite(bigram, word1, word2, sep = " ")

ngrams_5_1 <- ngrams_5_1 %>% 
  count(bigram, sort = TRUE)

```

```{r Sentiment 5_1}
na_5_1_claims_vector <- as.character(na_5_1_claims$text)
na_5_1_claims_sentiment <- get_nrc_sentiment(na_5_1_claims_vector)

na_5_1_claims_sentiment_score <- data.frame(colSums(na_5_1_claims_sentiment[,]))

names(na_5_1_claims_sentiment_score) <- 'score'

na_5_1_claims_sentiment_score <- cbind("sentiment" = rownames(na_5_1_claims_sentiment_score), na_5_1_claims_sentiment_score)

#rownames(training_sentiment_score) <- NULL

ggplot(na_5_1_claims_sentiment_score, aes(sentiment, score)) +
  geom_bar(aes(fill = sentiment), stat = "identity") +
  labs(x = "Sentiment", y = "Score", title = "Sentiment for Super Claim 5_1")
```

#Super Claim 5_2
Filter() for super claim 5
```{r}
na_5_2 <- nature_analysis %>%
  filter(str_detect(claim, "5_2"))
```

Add word_count column using mutate()
```{r}
na_5_2 <- na_5_2 %>% 
  mutate(word_count = str_count(na_5_2$text, "\\S+"))
```

Tokenize using unnest_tokens()
```{r}
na_5_2_tokenized <- nature_analysis %>% 
  unnest_tokens(words, text)

na_5_2_tokenized <- na_5_2_tokenized %>% 
  count(words) %>% 
  arrange(desc(n))
  
```

Filter() out stopwords()
```{r}
na_5_2_tokenized <- na_5_2_tokenized %>% 
  filter(!words %in% stopwords("english"))
```

```{r Word Cloud 5_2, warning=FALSE}
wordcloud(na_5_2_tokenized$words, freq = na_5_2_tokenized$n, max.words = 200, min.freq = 5, random.order = FALSE, random.color = FALSE, color = brewer.pal(12, "Paired"))
```

```{r frequencey matrix 5_2}
na_5_2_corpus <- corpus(na_5_2$text)

toks <- na_5_2_corpus %>%
    tokens(remove_punct = TRUE) %>%
    tokens_tolower() %>%
    tokens_remove(pattern = stopwords("english"), padding = FALSE)


fcmat <- fcm(toks, context = "window", tri = FALSE)

feat <- names(topfeatures(fcmat, 30))

fcm_select(fcmat, pattern = feat) %>%
    textplot_network(min_freq = 0.5)
```

```{r bigram 5_2}
na_5_2_claims <- na_5_2 %>% 
  select(text)

ngrams_5_2 <- na_5_2_claims %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

ngrams_5_2 <- ngrams_5_2 %>% 
  separate(bigram, c("word1", "word2"), sep = " ")

ngrams_5_2 <- ngrams_5_2 %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word)

ngrams_5_2 <- ngrams_5_2 %>% 
  unite(bigram, word1, word2, sep = " ")

ngrams_5_2 <- ngrams_5_2 %>% 
  count(bigram, sort = TRUE)

```

```{r Sentiment 5_2}
na_5_2_claims_vector <- as.character(na_5_2_claims$text)
na_5_2_claims_sentiment <- get_nrc_sentiment(na_5_2_claims_vector)

na_5_2_claims_sentiment_score <- data.frame(colSums(na_5_2_claims_sentiment[,]))

names(na_5_2_claims_sentiment_score) <- 'score'

na_5_2_claims_sentiment_score <- cbind("sentiment" = rownames(na_5_2_claims_sentiment_score), na_5_2_claims_sentiment_score)

#rownames(training_sentiment_score) <- NULL

ggplot(na_5_2_claims_sentiment_score, aes(sentiment, score)) +
  geom_bar(aes(fill = sentiment), stat = "identity") +
  labs(x = "Sentiment", y = "Score", title = "Sentiment for Super Claim 5_2")
```




#GGPPLOT comparisons
```{r}
united_sent_score <- full_join(na_1_sentiment_score, na_3_sentiment_score, by = "sentiment") %>% 
  full_join(na_5_claims_sentiment_score, by = "sentiment") %>% rename(claim_1 = score.x, claim_3 = score.y, claim_5 = score) #%>% 
 #transmute(sentiment, score = score.x + score.y + score)

```

```{r Sentiment Distribution}
ggplot(united_sent_score, aes(x = sentiment)) +
  geom_col(aes(y = claim_5, fill = "claim_5"), position = "stack") +
  geom_col(aes(y = claim_1, fill = "claim_1"), position = "stack") +
  geom_col(aes(y = claim_3, fill = "claim_3"), position = "stack") +
  #coord_flip() +
  #theme_wsj() +
  theme_minimal()+
  scale_fill_manual(values = c("claim_5" = "#08589E", "claim_1" = "#4EB3D3", "claim_3" = "#A8DDB5")) +
  #scale_fill_manual(values = c("claim_5" = "#FF7F0E", "claim_1" = "#2CA02C", "claim_3" = "#1F77B4")) +
  theme(text = element_text(family = "Arial", size = 21), axis.text.y = element_text(family = "Arial", size = 21), axis.title = element_text(family = "Arial", size = 30), plot.title = element_text(hjust = 0.5),
plot.subtitle = element_text(hjust = 0.5))+
  labs(x = "Sentiment", y = "Score", title = "Sentiment Scores", subtitle = "Super Claims 1, 3, & 5", fill = "Claim")
```

#Word Cloud
```{r Bigram Cloud 1, warning=FALSE}
wordcloud(ngrams_1$bigram, freq = ngrams_1$n, max.words = 200, min.freq = 5, random.order = FALSE, colors = c("royalblue1","seagreen2", "orangered"), family = "Avenir")
```

```{r Bigram Cloud 3, warning=FALSE}
wordcloud(ngrams_3$bigrams, freq = ngrams_3$n, max.words = 200, min.freq = 5, random.order = FALSE, colors = c("royalblue1","seagreen2", "orangered"), family = "Avenir")

```

```{r Bigram Cloud 5, warning=FALSE}
wordcloud(ngrams_5$bigram, freq = ngrams_5$n, max.words = 200, min.freq = 5, random.order = FALSE, color = brewer.pal(8, "Spectral"), family = "Avenir")

```

```{r Bigram Cloud 5_1, warning=FALSE}
wordcloud(ngrams_5_1$bigram, freq = na_5_1_tokenized$n, max.words = 200, min.freq = 5, random.order = FALSE, color = brewer.pal(8, "Dark2"), family = "Avenir")

```

```{r Bigram Cloud 5_2, warning=FALSE}
wordcloud(ngrams_5_2$bigram, freq = na_5_2_tokenized$n, max.words = 200, min.freq = 5, random.order = FALSE, color = brewer.pal(8, "Set1"), family = "Avenir")
```

#Bigram Frequencey Correlation
```{r Bigram 3 FC}
ngrams_3_corpus <- corpus(ngrams_3$bigrams)

toks <- na_3_corpus %>%
    tokens(remove_punct = TRUE) %>%
    tokens_tolower() %>%
    tokens_remove(pattern = stopwords("english"), padding = FALSE)


fcmat <- fcm(toks, context = "window", tri = FALSE)

feat <- names(topfeatures(fcmat, 30))

fcm_select(fcmat, pattern = feat) %>%
    textplot_network(min_freq = 0.5)
```

#Pyramid Plot
```{r Pyramid Plot Claims 1 & 3}
pyramid.plot(na_1_sentiment_score$score, na_3_sentiment_score$score, labels = na_1_sentiment_score$sentiment, main = "Sentiment Comparison for Claims 1 & 3", gap = 300, top.labels = c("Super Claim 1: CC is Not Happening", "Sentiment", "Super Claim 3: CC is Not Bad"), show.values = TRUE, unit = "Score", ppmar = c(8,4,8,4))
```

```{r Pyramid Plot Claims 1 & 5}
pyramid.plot(na_1_sentiment_score$score, na_5_claims_sentiment_score$score, labels = na_1_sentiment_score$sentiment, main = "Sentiment Comparison for Claims 1 and 5", top.labels = c("Super Claim 1: CC is Not Happening", "Sentiment", "Super Claim 5: Science/Scientist Not Reliable"), gap = 450, show.values = TRUE, unit = "Score", ppmar = c(8,4,8,4))
```

```{r Pyramid Plot Claims 3 & 5}
pyramid.plot(na_3_sentiment_score$score, na_5_claims_sentiment_score$score, labels = na_3_sentiment_score$sentiment, main = "Sentiment Comparison for Claims 3 and 5", top.labels = c("Super Claim 3: CC is Not Bad", "Sentiment", "Super Claim 5: Science/Scientist Not Reliable"), gap = 500, show.values = TRUE, unit = "Score", ppmar = c(8,4,8,4))
```

right_join() to combine sentiment scores for super-claims 1 and 3
select() relevant columns to rename()
transmute() to select "Sentiment" column and creat a new column named "Score" that is the sums of Score_1 + Score_3
```{r Combining Claims 1 & 3}

sentiment_1_3 <- right_join(na_1_sentiment_score, na_3_sentiment_score, by = "sentiment", keep = TRUE)

sentiment_1_3 <- sentiment_1_3 %>% 
  select(sentiment.x, score.x, score.y) %>% 
  rename(Sentiment = sentiment.x, Score_1 = score.x, Score_3 = score.y) %>%
  ungroup() %>% 
 transmute(Sentiment, Score = Score_1 + Score_3)

```

```{r Pyramid Plot 1 +3  & 5}
pyramid.plot(sentiment_1_3$Score, na_5_claims_sentiment_score$score, labels = na_5_claims_sentiment_score$sentiment, main = "Sentiment Comparison for Combined Claims 1 + 3 & 5", top.labels = c("Super Claim 1 & 3", "Sentiment", "Super Claim 5: Science/Scientist Not Reliable"), gap = 500, show.values = TRUE, unit = "Score", ppmar = c(8,4,8,4))
```

```{r Pyramid Plot 5_1 & 5_2}
pyramid.plot(na_5_1_claims_sentiment_score$score, na_5_2_claims_sentiment_score$score, labels = na_5_1_claims_sentiment_score$sentiment, main = "Sentiment Comparison for Super Claim 5", top.labels = c("Super Claim 5_1:Science is Unreliable", "Sentiment", "Super Claim 5_2:Movement is Unreliable"), gap = 450, show.values = TRUE, unit = "Score", ppmar = c(8,4,8,4))
```

```{r pyramid plot 1, eval=FALSE, include=FALSE}
cw_1_5 <- full_join(na_1_tokenized, na_3_tokenized, by = "words") 

commonwords <- subset (cw_1_5, cw_1_5[, 2] > 0 & cw_1_5[, 3] > 0)

#commonwords <- commonwords %>%
  #arrange(desc(n.x))

difference <- abs(commonwords[, 2] - commonwords[, 3])

commonwords <- cbind(commonwords, difference)
commonwords <- commonwords[order (commonwords[, 4],
decreasing = TRUE), ]
top25_df <- data.frame(x = commonwords [1:25, 1],
y = commonwords [1:25, 2],
labels = rownames (commonwords [1:25, ]))

colnames(top25_df) <- c("labels", "x", "y")

top25_df$x <- as.numeric(top25_df$x)
top25_df$y <- as.numeric(top25_df$y)

pyramid.plot(top25_df$x, top25_df$y,
             labels = top25_df$labels,
             main = "Words in Common",
             gap = 8)


```

```{r pyramid plot 2, eval=FALSE, include=FALSE}
cw_1_5 <- full_join(na_1_tokenized, na_3_tokenized, by = "words") 

commonwords <- subset (cw_1_5, cw_1_5[, 2] > 0 & cw_1_5[, 3] > 0)

commonwords <- commonwords[1:39, ]

difference <- abs(commonwords[, 2] - commonwords[, 3])

commonwords <- cbind(commonwords, difference)
commonwords <- commonwords[order (commonwords[, 4],
decreasing = TRUE), ]



top25_df <- data.frame(
  x = commonwords [1:25, 1],
  y = commonwords [1:25, 2],
  z = commonwords [1:25, 3],
  a = commonwords [1:25, 4]
  )



colnames(top25_df) <- c("labels", "x", "y")

top25_df$x <- as.numeric(top25_df$y)
top25_df$y <- as.numeric(top25_df$z)

pyramid.plot(top25_df$x, top25_df$y,
             labels = top25_df$labels,
             main = "Words in Common",
             gap = 8, 
             top.labels = c("NA_1", 
                            "Common Words", 
                            "NA_3"))
```








#Comparison Cloud Plot
```{r prep, eval=FALSE, include=FALSE}
na_1_matp <- na_1_claims %>% 
  select(text)

na_3_matp <- na_3_claims %>% 
  select(text)

na_5_matp <- na_5_claims %>% 
  select(text)

na_1_matrix <- as.matrix.data.frame(na_1_matp)
na_3_matrix <- as.matrix.data.frame(na_3_matp)
na_5_matrix <- as.matrix.data.frame(na_5_matp)

na_1_text <- apply(na_1_matrix, 1, toString)
na_3_text <- apply(na_3_matrix, 1, toString)
na_5_text <- apply(na_5_matrix, 1, toString)






na_list <- list(na_1_text, na_3_text, na_5_text)

na_list <- lapply(na_list, as.vector.data.frame)
unlist(na_list)

```


```{r Comparison Cloud, eval=FALSE, include=FALSE}
if (require(tm)) {
  # Replace the following code with your own text data
  texts <- (na_list)
  
  # Create a corpus from the text data
  corp <- Corpus(VectorSource(texts))
  
  # Preprocess the corpus
  corp <- tm_map(corp, removePunctuation)
  corp <- tm_map(corp, content_transformer(tolower))
  corp <- tm_map(corp, removeNumbers)
  corp <- tm_map(corp, removeWords, stopwords())
  
  # Create the term document matrix
  term.matrix <- DocumentTermMatrix(corp)
  term.matrix <- as.matrix(term.matrix)
  
  # Assign column names to the matrix
  colnames(term.matrix) <- paste0("Document ", 1:ncol(term.matrix))
  
  # Generate the word cloud
  comparison.cloud(term.matrix, max.words = 200, random.order = FALSE,)
  comparison.cloud(term.matrix, max.words = 200, random.order = FALSE,
                   title.colors = c("red", "blue"), title.bg.colors = c("grey40", "grey70"))
  comparison.cloud(term.matrix, max.words = 200, random.order = FALSE,
                   match.colors = TRUE)
  break
}

```
#Word Associate Plot////NEEDS JAVA
```{r Word Associate NA_1, eval=FALSE, include=FALSE}
word_associate(na_1$text, match.string = c("climate"), stopwords = stop_words$word, network.plot = TRUE, cloud.colors = c("gray85", "darkred"))
```
#Other
```{r eval=FALSE, include=FALSE}
ngrams10 <- na_5_claims %>% 
  unnest_tokens(fourgram, text, token = "ngrams", n = 4)


ngrams10 <- ngrams10 %>% 
 separate(fourgram, c("word1", "word2", "word3", "word4"), sep = " ") 

ngrams10 <- ngrams10 %>%
  filter(!word1 %in% stop_words$word) %>%
      filter(!word2 %in% stop_words$word) %>% 
        filter(!word3 %in% stop_words$word) %>% 
          filter(!word4 %in% stop_words$word)


ngrams10 <- ngrams10 %>%
  unite(fourgram, word1, word2, word3, word4, sep=" ")

ngrams10_count <- ngrams10 %>% 
  count(fourgram, sort = TRUE)

head(ngrams10_count)
```
